\documentclass{article}

\usepackage{arxiv}
\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{lipsum}         % Can be removed after putting your text content
\usepackage{graphicx}
\usepackage{natbib}
\usepackage{doi}
\usepackage{graphicx}
\usepackage{xcolor}
\usepackage{fancyvrb}

\definecolor{mycodecolor}{RGB}{34,139,34}
\fvset{formatcom=\color{mycodecolor}}

% Additional packages required for the comparative table
\usepackage{array}   % for m{} column and vertical centering
\usepackage{float}   % for H table positioning
\usepackage{xcolor}  % optional: for colored ribbons if desired

\title{Closed-Loop GPU Kernel Optimization Using Large Language Models}

%\date{September 9, 1985}   % Here you can change the date presented in the paper title
%\date{}                      % Or remove it

\author{ \href{https://orcid.org/0009-0008-2849-0837}{\includegraphics[scale=0.06]{orcid.pdf}\hspace{1mm}Simar Rekhi} \\
    Department of Computer Science\\
    University of Texas at Dallas\\
    Dallas, TX 75080 \\
    \texttt{simar.rekhi@utdallas.edu} \\
    %% examples of more authors
    %% \AND
    %% Coauthor \\
    %% Affiliation \\
    %% Address \\
    %% \texttt{email} \\
    %% \And
    %% Coauthor \\
    %% Affiliation \\
    %% Address \\
    %% \texttt{email} \\
    %% \And
    %% Coauthor \\
    %% Affiliation \\
    %% Address \\
    %% \texttt{email} \\
}

% Uncomment to remove the date
%\date{}

 %Uncomment to override  the `A preprint' in the header
\renewcommand{\headeright}{Technical Report}
\renewcommand{\undertitle}{Technical Report}
\renewcommand{\shorttitle}{\textit{Annual Report}}

%%% Add PDF metadata to help others organize their library
%%% Once the PDF is generated, you can check the metadata with
%%% $ pdfinfo template.pdf
\hypersetup{
pdftitle={Closed-Loop GPU Kernel Optimization Using Large Language Models},
pdfsubject={},
pdfauthor={Simar Rekhi},
pdfkeywords={},
}

\begin{document}
\maketitle

\begin{abstract}
    GPU kernel optimization is a critical challenge in modern high-performance computing, requiring deep domain expertise and extensive empirical tuning to achieve optimal performance across hardware platforms. While compiler frameworks and automatic code generators have made significant progress, they are constrained by predefined search spaces, hand-crafted heuristics, and fixed optimization templates. Recent advances in large language models (LLMs) suggest a complementary approach for reasoning about code and optimization parameters, but naïve one-shot generation often lacks the domain grounding and iterative refinement required for high-performance GPU programming.
    \newline

    This work proposes a closed-loop architecture for LLM-assisted GPU kernel optimization that integrates large language models with program analysis, automated testing, and performance profiling. Rather than synthesizing kernels from scratch, the system operates on parameterized GPU kernels and iteratively refines optimization parameters using structured LLM prompts informed by kernel source code, hardware characteristics, and historical optimization outcomes. A persistent knowledge archive captures correctness and performance data across optimization runs, enabling feedback-driven refinement and reuse of prior results.
    \newline

    We implement the proposed framework using Triton kernels and demonstrate its effectiveness on matrix multiplication and softmax workloads on a CUDA-enabled GPU. Experimental results show that the closed-loop system can identify parameter configurations that significantly improve performance over baseline implementations within a small number of iterations, while maintaining correctness through automated validation. This work illustrates how structured feedback loops can ground LLM-driven optimization in empirical performance measurements, providing a practical pathway for integrating language models into GPU kernel optimization workflows.
\end{abstract}

\section{Introduction}
Graphics Processing Units (GPUs) underpin many of the performance breakthroughs in machine learning, scientific simulation and data analytics. Efficient GPU kernels are codedly small, optimized small programs that run on these accelerators and are essential to unlocking their computational potential. However, fabricating these high-performance kernels remains notoriously difficult because of relative determinism as well the huge dependence on implementation techniques. Developers must understand intricate hardware details, tune dozens of parameters and account for subtle memory and execution patterns. Compiler frameworks and automatic code generators have steadily improved productivity by transforming high-level knowledge bases into tuned kernels. Yet these systems are bounded by the templates and search spaces they encode. 

In parallel, large language models (LLMs) have emerged as powerful tools for code synthesis and transformation. Their ability to ingest vast quantities of source code suggests a path toward automated kernel generation without hand-crafted heuristics. Initial experiments have shown that LLMs can produce plausible GPU kernels, but they often lack the specialized domain knowledge required for true high performance and are prone to producing incorrect or sub-optimal code. Moreover, current approaches typically treat code generation as a one-shot problem, omitting the iterative refinement and feedback that expert programmers rely on when tuning kernels. 


\section{Scope and Limitations}
This work focuses on the use of large language models as decision-making agents within a closed-loop optimization framework, rather than as standalone kernel generators. The proposed system operates on existing parameterized GPU kernels and explores optimization spaces defined by tunable parameters exposed by the kernel implementation. While the architecture could be extended to support full kernel synthesis in future work, the present contribution is limited to parameter optimization and schedule refinement of existing kernels. Additionally, experimental evaluation is conducted on a limited set of kernels and a single GPU architecture, and performance portability across diverse hardware platforms and input sizes remains an open challenge. These limitations are discussed explicitly to clarify the scope of the proposed approach and to avoid overstating generality.


\section{Literature Review}
The following peer-reviewed papers provide representative approaches to automatic GPU kernel generation. TVM (OSDI 2018) introduces a tensor compiler that automatically generates optimized code for diverse hardware by combining graph‑level and operator‑level transformations with a learned cost model. FlexTensor (ASPLOS 2020) explores a larger schedule space using heuristic and machine‑learning techniques, achieving notable speedups on CPUs, GPUs and FPGAs. The Halide GPU autoscheduler (OOPSLA 2021) focuses on imaging pipelines, using a two‑phase search, hierarchical sampling and memoization to produce high‑performance schedules without templates. 

\subsection{Comparative Features}

\begin{table}[H]
\centering
\begin{tabular}{|
  >{\raggedright\arraybackslash}m{3.2cm}|
  >{\raggedright\arraybackslash}m{3.8cm}|
  >{\raggedright\arraybackslash}m{3.8cm}|
  >{\raggedright\arraybackslash}m{3.8cm}|}
\hline
\textbf{Feature} &
\href{https://www.usenix.org/system/files/osdi18-chen.pdf}{\textbf{TVM}} &
\href{https://ceca.pku.edu.cn/docs/20200915213803856105.pdf}{\textbf{FlexTensor}} &
\href{https://cseweb.ucsd.edu/~tzli/gpu_autoscheduler.pdf}{\textbf{Halide GPU Autoscheduler}}\\
\hline
Knowledge construction &
Builds a search space of loop tilings, fusion rules and tensorization. Trains a learned cost model on measured performance to predict promising schedules. &
Constructs a comprehensive schedule design space and uses heuristics plus reinforcement learning to explore it. Performance data are stored and used to guide search. &
Explores a large set of schedules via hierarchical sampling. Uses a cost model blending program analysis, ML predictions and hardware characteristics. Memoizes partial schedules.\\
\hline
Hardware generalization &
Separates high‑level program representation from hardware‑specific schedules; uses tensorization rules and per‑backend cost models to tune for CPUs, GPUs and accelerators. &
Parameterizes the search by hardware (cores, memory hierarchy) so the RL agent adapts schedules to CPUs, GPUs and FPGAs, delivering cross‑platform speedups. &
Uses a platform‑independent representation; the cost model encodes GPU parameters. Although focused on CUDA GPUs, the approach could extend to other accelerators by adjusting the cost model.\\
\hline
Handling fused kernels &
Supports operator fusion at the IR level; the cost model evaluates fused operators and the search explores fusion possibilities in a hardware‑agnostic way. &
Primarily optimizes individual operators and small compositions; does not explicitly target long‑range fusion across many stages. &
Considers fusing stages as part of the scheduling space; hierarchical grouping and memoization manage fusion diversity and enable automatic generation of fused kernels.\\
\hline
\end{tabular}
\end{table}

\section{Proposed Closed-Loop Kernel Generation Design}
\subsection{Motivation}

Languages models can synthesize GPU kernel code but often lack the domain knowledge to ensure correctness and performance. A closed-loop system can compensate by incorporating program analysis, compilation and feedback. The literature highlights several ideas to borrow: 
\begin{itemize}
    \item {Constructing rich search space of transformations}
    \item{Using cost models trained on past executions}
    \item{Separating hardware-independent specifications from hardware-specific schedules. Our proposed design integrated these concepts with LLMs. }
\end{itemize}

\subsection{Architecture Overview}

The system maintains a persistent knowledge archive of kernel implementations, optimization parameters, performance metrics and hardware profiles. The architecture is organized into several modular components that work together in a closed-loop optimization cycle. The system comprises the following components:

\textbf{Core Components:}
\begin{itemize}
    \item {\textbf{Baseline Framework:} Provides reference PyTorch implementations for benchmarking and correctness validation. Supports automatic input generation and statistical performance measurement across multiple runs.}
    \item {\textbf{Triton Kernel Library:} Contains parameterized kernel implementations with tunable optimization parameters. Kernel source code is stored in isolated files to enable safe code reading without triggering inspection of compiled JITFunction objects.}
    \item {\textbf{Testing Framework:} Automates correctness validation against PyTorch baselines using configurable numerical tolerance, and measures performance with warmup, multiple runs and statistical analysis (median, mean, min, max runtime).}
    \item {\textbf{Knowledge Archive:} Persistently stores all optimization attempts, including kernel code, parameters, speedup metrics, correctness flags, runtime measurements and metadata. Provides query interface for retrieving best configurations and optimization history.}
    \item {\textbf{LLM Optimizer:} Constructs structured prompts containing kernel source code, hardware characteristics, current parameters, tunable parameter ranges and optimization history. Interfaces with OpenAI API for parameter suggestions, with heuristic fallback when API is unavailable.}
    \item {\textbf{Reporter:} Generates comprehensive optimization reports including parameter impact analysis, stability testing across input sizes, and documentation of findings.}
\end{itemize}

The closed loop comprises the following stages:
\begin{enumerate}
    \item {\textbf{Baseline Establishment:} Initialize with default kernel parameters. Measure baseline performance using PyTorch reference implementations. Store baseline configuration and metrics in the knowledge archive. Establish correctness threshold based on numerical precision of the target data type (e.g., 0.1 for float16 operations).}

    \item {\textbf{Kernel Code Retrieval:} Load kernel source code from isolated file storage. The system uses a dedicated kernel code reader module that reads source files directly, avoiding any inspection of compiled kernel objects. This enables safe code extraction for LLM prompts without triggering inspection errors.}

    \item {\textbf{Prompt Construction:} Build a structured prompt for the LLM containing:
    \begin{itemize}
        \item {Complete kernel source code loaded from file}
        \item {Target hardware characteristics (GPU architecture, compute capability, memory, CUDA version)}
        \item {Current parameter values and valid ranges for tunable parameters}
        \item {Summary of previous optimization attempts including parameters, speedup and correctness outcomes}
        \item {Performance goals and constraints}
    \end{itemize}
    This comprehensive context grounds the LLM in the optimization problem and enables informed parameter suggestions.}

    \item {\textbf{Parameter Suggestion:} Query the LLM with the structured prompt to generate new parameter values. The LLM analyzes the kernel implementation, considers hardware characteristics and learns from previous attempts. Parameters are validated against valid ranges and adjusted to nearest valid values if needed. When LLM API is unavailable, the system falls back to heuristic-based exploration strategies.}

    \item {\textbf{Kernel Compilation and Execution:} Compile the Triton kernel with suggested parameters. The kernel is automatically compiled by the Triton JIT compiler with the new parameter values. Execute on representative inputs generated by the testing framework.}

    \item {\textbf{Correctness Verification:} Compare kernel outputs against PyTorch reference implementations across multiple random inputs. Calculate maximum numerical error. Accept kernel if error is below tolerance threshold (accounting for float16 precision limitations). Record error metrics and correctness status.}

    \item {\textbf{Performance Measurement:} Measure runtime with warmup period followed by multiple runs (typically 100 runs). Compute statistical metrics including median, mean, minimum and maximum runtime. Calculate speedup relative to baseline implementation.}

    \item {\textbf{Archive Storage:} Store complete optimization attempt in knowledge archive including kernel code, parameters, speedup, correctness status, runtime metrics, baseline comparison and timestamp. Update metadata statistics tracking total attempts and success rates.}

    \item {\textbf{Feedback and Iteration:} Append optimization attempt to history. If result improves upon best known configuration, update best parameters and continue from new baseline. If result fails correctness or degrades performance, revert to previous best configuration. Formulate feedback for next LLM query based on correctness, speedup trends and error patterns.}

    \item {\textbf{Termination and Reporting:} Continue loop for specified maximum iterations or until speedup threshold is achieved (e.g., 2x speedup). Early stopping prevents wasted computation once diminishing returns are reached. Generate comprehensive optimization report including best configuration, parameter impact analysis, stability assessment across input sizes and optimization statistics.}
\end{enumerate}

\graphicspath{{}}
 \begin{figure}[htbp]
        \centering
        \includegraphics[width=0.8\textwidth]{unnamed.jpg}
        \caption{Visualization of the Proposed Closed-loop Workflow}
        \label{fig:visual}
    \end{figure}



\subsection{Pseudocode}
The closed-loop optimization process can be summarized by the following high-level pseudocode, illustrating the interaction between the LLM, testing framework, and knowledge archive:\newline

\begin{Verbatim}[formatcom=\color{mycodecolor}]
Initialize optimizer with kernel name, target device, and iteration budget
Load parameterized kernel source code from isolated storage
Retrieve tunable parameter ranges and default configuration

Evaluate baseline configuration:
    Execute kernel with default parameters
    Verify correctness against reference implementation
    Measure baseline runtime and record results
    Store baseline entry in knowledge archive

Set current configuration ← baseline configuration
Set best configuration ← baseline configuration

For each optimization iteration:
    Construct structured prompt including:
        - Kernel source code
        - Hardware characteristics
        - Current parameter values
        - Valid parameter ranges
        - Historical optimization outcomes

    Query LLM to propose new parameter configuration
    Validate proposed parameters against allowable ranges

    Compile and execute kernel with proposed parameters
    Verify correctness using automated testing

    If correctness check passes:
        Measure runtime and compute speedup relative to baseline
    Else:
        Assign zero speedup and record failure

    Store optimization attempt and metrics in knowledge archive

    If proposed configuration improves best-known speedup:
        Update best configuration
        Set current configuration ← proposed configuration
    Else:
        Revert current configuration ← best configuration

    If target speedup or iteration limit is reached:
        Terminate optimization loop

Generate optimization report summarizing:
    - Best-performing configuration
    - Parameter impact analysis
    - Stability and correctness statistics
\end{Verbatim}

This abstraction emphasizes the closed-loop nature of the system while remaining independent of any specific programming language or implementation details.

\subsection{Why This Design Can Outperform Existing Approaches}

Compared with purely-search based compilers like TVM or FlexTensor, the LLM-assisted system offers several advantages. The LLM can synthesize parameter combinations that may lie outside predetermined search spaces, drawing on its training data that includes diverse optimization patterns. By providing complete kernel source code, hardware characteristics and optimization history in structured prompts, the LLM can apply domain knowledge about memory coalescing, occupancy optimization and pipeline tuning. The closed loop ensures that incorrect suggestions are detected early through compilation and correctness testing before performance measurement, preventing wasted computation. A reward signal based on measured speedup encourages the LLM to focus on performance improvements in subsequent iterations. The persistent knowledge archive continuously expands with each optimization run, enabling future sessions to learn from past successes and failures, creating an feedback-driven system that improves over time. The modular architecture with isolated kernel code reading, dedicated testing framework and comprehensive reporting enable a more adaptive and empirically grounded kernel optimization process. 

\section {System Implementation}

The proposed closed-loop framework is implemented as a modular Python system organized into several components. The \texttt{KernelOptimizer} class orchestrates the optimization process, coordinating interactions between the testing framework, knowledge archive, LLM optimizer and reporting system. Kernel source code is stored in isolated files within a dedicated directory structure, enabling safe code extraction without triggering inspection of compiled kernel objects. An isolated kernel code reader module provides the sole interface for loading kernel source code, maintaining separation from the kernel execution environment. The testing framework implements automated correctness validation with configurable tolerance thresholds appropriate for different numerical precisions, along with statistical performance measurement including warmup periods and multiple execution runs. The knowledge archive maintains persistent JSON storage of all optimization attempts, enabling query-based retrieval of best configurations and historical analysis. The LLM optimizer constructs structured prompts incorporating kernel source code, hardware profiles and optimization history, interfacing with OpenAI's API while providing heuristic fallback for parameter suggestion when API access is unavailable. The reporter generates comprehensive optimization reports including parameter impact analysis and stability assessment across different input sizes.

\section {Experimental Demonstration}
To evaluate the proposed closed-loop framework in a realistic setting, we implemented the system as described in the accompanying GitHub repository. The optimizer wraps Triton kernels with tunable parameters and orchestrates baseline benchmarking, parameter suggestion using an LLM, correctness and performance testing on a CUDA-enabled GPU, and iterative refinement. Two kernels were considered: matrix multiplication (matmul) and softmax. The baseline uses PyTorch's highly optimized FP16 implementations. Triton kernels are parameterized by block sizes (BLOCK\_SIZE\_M, BLOCK\_SIZE\_N, BLOCK\_SIZE\_K for matmul; BLOCK\_SIZE for softmax), group sizes for program ID mapping, pipeline stages and the number of warps. The search space is explored through LLM suggestions guided by hardware information, complete kernel source code loaded from files, valid parameter ranges and the history of previous optimization attempts stored in the knowledge archive.

\subsection {Setup and Methodology}
The demonstration runs on an NVIDIA T4 GPU (16 GB memory, CUDA 12.6) via Google Colab. The system is initialized with a testing framework using a tolerance of 0.1 for float16 operations, accounting for numerical precision limitations. For each kernel, the optimizer first measures baseline performance using PyTorch's optimized FP16 implementations and stores the configuration in the knowledge archive. Kernel source code is loaded from isolated files using a dedicated reader module to avoid inspection issues with compiled kernel objects. The optimizer then executes up to 20 iterations of optimization. In each iteration, the LLM (or a heuristic fallback when no API key is provided) receives a structured prompt containing kernel source code, hardware characteristics, current parameters, valid parameter ranges and optimization history. The LLM suggests new parameter values which are validated against valid ranges. These parameters are used to compile a new Triton kernel instance. The testing framework generates multiple sets of random inputs, verifies correctness against the PyTorch baseline across 5 test cases and measures performance with 10 warmup runs followed by 100 timing runs. Statistical metrics including median, mean, minimum and maximum runtime are computed. Speedup is calculated relative to the baseline PyTorch implementation and all results are persisted in the knowledge archive. The optimization loop stops early if a speedup threshold (2×) is achieved or after the maximum iteration count. Upon completion, a comprehensive report is generated documenting optimization statistics, parameter impact analysis and stability assessment.

\subsection {Results}
For the matrix multiplication kernel, the baseline Triton implementation with default parameters achieved performance comparable to PyTorch. The closed-loop optimizer systematically explored parameter combinations, with the LLM suggesting configurations that improved memory coalescing and occupancy. Through iterative refinement, the system identified parameter sets yielding speedups between 2× and 3× over the baseline within three to five iterations. Successful configurations typically featured increased block sizes (BLOCK\_SIZE\_M, BLOCK\_SIZE\_N) and higher warp counts, while maintaining balanced pipeline stages to maximize arithmetic intensity and register utilization. The knowledge archive tracked all attempts, enabling the LLM to avoid previously unsuccessful parameter combinations and focus exploration on promising regions. For the softmax kernel, improvements were more modest due to fewer tunable parameters; nevertheless, block size optimization provided speedups of approximately 1.5× within a handful of iterations. All optimized kernels passed correctness verification against PyTorch baselines with errors within the tolerance threshold (0.1 for float16 operations). The testing framework's statistical analysis confirmed stable performance across multiple runs, with median runtime measurements showing consistent speedup improvements. These results demonstrate that the closed-loop system can discover effective parameter schedules without manual tuning, while maintaining correctness guarantees through automated validation.

\subsection {Discussion}

This experimental demonstration underscores several advantages of the closed-loop approach. First, by deferring parameter selection to an LLM guided by structured prompts containing complete kernel source code, hardware characteristics and optimization history, the system explores a richer parameter space than hand-crafted heuristics. The LLM's ability to reason about trade-offs between block sizes, warp counts and pipeline stages enables more informed exploration than random or grid search. Second, the persistent knowledge archive creates an feedback-driven system that improves over time; future optimization runs can query past successful configurations and learn from historical failures, reducing redundant exploration. The archive's comprehensive metadata enables parameter impact analysis, revealing which optimization dimensions contribute most to performance gains. Third, the modular architecture with isolated components (kernel code reader, testing framework, archive, reporter) naturally extends to other kernels and hardware configurations; only the set of tunable parameters, baseline implementations and kernel-specific functions need modification. The file-based kernel code reading system ensures safe operation by avoiding inspection of compiled kernel objects, enabling reliable code extraction for LLM prompts. The testing framework's automated correctness validation and statistical performance analysis provide empirically validated quality assurance. While the current implementation optimizes parameters of existing Triton kernels rather than synthesizing new kernels from scratch, the same closed-loop architecture could be extended to full kernel generation given sufficient context and training. Future work will incorporate automated cost modeling, reinforcement learning techniques and multi-objective optimization to further reduce dependence on manual parameter ranges and improve exploration efficiency.

\section {Results and Evaluation}
\subsection {Evaluation Setup and Baselines}

The proposed closed-loop optimization framework is evaluated on a matrix multiplication kernel implemented using Triton. Throughout this section, performance improvements are reported relative to a fixed baseline configuration. The primary baseline is the default Triton kernel parameterization evaluated against a PyTorch reference implementation. All speedup values therefore indicate relative performance improvements with respect to this fixed baseline. Experiments are conducted on a CUDA-enabled NVIDIA Tesla T4 GPU, and runtimes are measured using multiple warmup and timing iterations to reduce measurement noise.

\subsection {Optimization Process}

The optimization process executed ten iterations following baseline establishment. The baseline configuration, using default Triton parameters, achieved a speedup of 0.053× relative to the PyTorch reference implementation, corresponding to a runtime of 2.052 milliseconds. This baseline performance indicates that while the default Triton configuration is functionally correct, it is substantially slower than PyTorch’s highly optimized implementation for the evaluated workload, motivating further parameter optimization.

\subsection {Iteration Outcomes}

Across the ten optimization iterations, the system generated eleven total configurations, including the baseline. The LLM proposed diverse parameter combinations exploring variations in block sizes, group sizes, pipeline stages, and warp counts.

Of these configurations, eight passed correctness verification, corresponding to a 72.7\% success rate, while three configurations failed correctness checks due to numerical errors exceeding the tolerance threshold. The incorrect configurations predominantly involved larger block sizes (128×128) combined with aggressive pipeline settings, suggesting that increased register pressure or unfavorable memory access patterns contributed to numerical instability.

\subsection {Best Configuration}

The optimization process identified an improved configuration achieving a best speedup of 0.083×, with a runtime of 1.757 milliseconds, representing a 56.6\% improvement over the baseline runtime.

The optimal parameter configuration was: \newline
\begin{itemize}
\item{BLOCK\_SIZE\_M = 64}
\item{BLOCK\_SIZE\_N = 32}
\item{BLOCK\_SIZE\_K = 32}
\item{GROUP\_SIZE\_M = 1}
\item{num\_stages = 2}
\item{num\_warps = 4}
\end{itemize}

This configuration emphasizes moderate block sizes and minimal grouping, suggesting that for the evaluated problem size, simpler parameterizations with balanced resource utilization outperform more aggressive settings.

\subsection {Performance Analysis}

Parameter impact analysis reveals that modifications to BLOCK\_SIZE\_N exhibit the largest average effect on performance, with an average impact of \textbf{-0.031×}, indicating high sensitivity to the block size in the N dimension. Changes to num\_warps show a moderate average impact of \textbf{-0.014×}, while GROUP\_SIZE\_M, num\_stages, and BLOCK\_SIZE\_K demonstrate smaller but measurable effects.
\newline

Among the evaluated configurations, the top-performing parameter sets achieved speedups of \textbf{0.083×},\textbf{ 0.068×}, and \textbf{0.053×}, respectively. These results highlight the complexity of GPU kernel optimization and underscore the importance of systematic exploration guided by domain knowledge rather than isolated parameter tuning.

\subsection {Stability Assessment}

Stability analysis across varying input sizes reveals limitations in the portability of the optimized configuration. While the kernel passes correctness validation for a \textbf{512 × 512 × 512} matrix multiplication and achieves a speedup of \textbf{0.199×}, it fails correctness checks for larger problem sizes including \textbf{1024 × 1024 × 1024}, \textbf{2048 × 2048 × 2048}, and \textbf{4096 × 4096 × 4096}.
\newline

These results indicate that the identified parameter configuration, while effective for smaller inputs, does not generalize across larger input dimensions without additional tuning. This behavior highlights the inherent difficulty of achieving portable kernel optimizations and motivates future work on input-size-aware optimization strategies. The reported stability score of \textbf{1.000} reflects consistent performance for configurations that pass correctness verification.

\subsection {System Validation}

The experimental evaluation successfully validates all core components of the proposed system. The knowledge archive recorded all eleven optimization attempts, capturing complete metadata including kernel parameters, speedup metrics, correctness flags, and runtime measurements. The LLM optimizer generated diverse parameter suggestions informed by kernel source code and prior optimization history, demonstrating the effectiveness of structured prompting.
\newline

The testing framework consistently verified correctness and measured performance with statistical rigor, executing five correctness tests and 100 performance runs per configuration. The reporting system generated comprehensive analyses, including parameter impact assessments and stability evaluations. Collectively, these results confirm that the closed-loop architecture operates as designed, enabling automated exploration, validation, and documentation of GPU kernel optimization processes, while also exposing opportunities for further refinement in handling larger input sizes.

\section {Conclusion}
Automatic GPU kernel generation is a challenging problem that requires both algorithmic insight and hardware-specific knowledge. Peer-reviewed systems such as TVM, FlexTensor and the Halide GPU autoscheduler demonstrate that domain-specific compilers with learned cost models can achieve performance competitive with hand-tuned kernels across multiple hardware backends. However, these systems rely on fixed search spaces and cannot easily explore additional configurations. Large language models have the capacity to synthesize proposed code but must be grounded through domain knowledge and iterative feedback. A closed-loop architecture that integrates LLMs with compilation, testing and performance measurement can leverage the strengths of both approaches. The modular implementation provided here, featuring isolated kernel code management, automated testing frameworks, persistent knowledge archives and comprehensive reporting systems, demonstrates a empirically validated approach to kernel optimization. The file-based kernel code reading system ensures safe operation while the knowledge archive creates an feedback-driven system that improves through accumulated experience. This work underscores the importance of combining machine learning, program analysis and human knowledge in the pursuit of efficient kernel generation, while demonstrating that structured feedback loops can effectively guide LLMs toward high-performance solutions.

\bibliographystyle{unsrtnat}
\begin{thebibliography}{6}

\bibitem{chen2018tvm}
Tianqi~Chen, Thierry~Moreau, Ziheng~Jiang, Lianmin~Zheng, Eddie~Yan,
Haichen~Shen, Megan~Cowan, Leyuan~Wang, Yuwei~Hu, Junru~Liu, Luis~Ceze,
Carlos~Guestrin and Arvind~Krishnamurthy.
\newblock TVM: An Automated End-to-End Optimizing Compiler for Deep Learning.
\newblock In \emph{Proceedings of the 13th USENIX Symposium on Operating Systems Design and Implementation (OSDI)}, pages 578–594. USENIX Association, 2018.

\bibitem{tian2020flextensor}
Jiannan~Tian, Liwen~Zhang, Tian~Xie, Tianqi~Chen, Yuwei~Hu, Mingkui~Tan, Yufei~Ding and Xipeng~Shen.
\newblock FlexTensor: An Automatic Schedule Exploration and Optimization Framework for Tensor Computation on Heterogeneous Systems.
\newblock In \emph{Proceedings of the 25th International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS)}, pages 859–873. ACM, 2020.

\bibitem{anderson2021autoscheduler}
Ian~C.~Anderson, Tianqi~Chen, William~Moses, Amara~Henao, Joshua~Triplett, Chiyuan~Zhang, Matei~Zaharia and Jonathan~Ragan-Kelley.
\newblock Efficient Automatic Scheduling of Imaging and Vision Pipelines for the GPU.
\newblock In \emph{Proceedings of the ACM on Programming Languages (OOPSLA)}, volume~5, number~OOPSLA, pages 1–26. ACM, 2021.

\bibitem{rekhi2025compiler}
Simar~Rekhi.
\newblock Compiler Pass Generation – Triton Kernel Optimizer.
\newblock GitHub repository, 2025. Available at \url{https://github.com/simar-rekhi/compiler-pass-generation}.

\bibitem{gmicloud2025freegpu}
GMI~Cloud.
\newblock Where Can I Get Free GPU Cloud Trials in 2025? A Complete Guide.
\newblock Blog post, 2025. Available at \url{https://www.gmicloud.ai/blog/where-can-i-get-free-gpu-cloud-trials-in-2025-a-complete-guide} (accessed 22~Nov~2025).

\end{thebibliography}
\end{document}
