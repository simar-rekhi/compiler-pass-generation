\documentclass{article}

\usepackage{arxiv}
\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{lipsum}         % Can be removed after putting your text content
\usepackage{graphicx}
\usepackage{natbib}
\usepackage{doi}
\usepackage{xcolor}
\usepackage{fancyvrb}

\definecolor{mycodecolor}{RGB}{34,139,34}
\fvset{formatcom=\color{mycodecolor}}

% Additional packages required for the comparative table
\usepackage{array}   % for m{} column and vertical centering
\usepackage{float}   % for H table positioning

\title{Closed-Loop GPU Kernel Optimization Using Large Language Models}

%\date{September 9, 1985}   % Here you can change the date presented in the paper title
%\date{}                      % Or remove it

\author{ \href{https://orcid.org/0009-0008-2849-0837}{\includegraphics[scale=0.06]{orcid.pdf}\hspace{1mm}Simar Rekhi} \\
    Department of Computer Science\\
    University of Texas at Dallas\\
    Dallas, TX 75080 \\
    \texttt{simar.rekhi@utdallas.edu} \\
    %% examples of more authors
    %% \AND
    %% Coauthor \\
    %% Affiliation \\
    %% Address \\
    %% \texttt{email} \\
    %% \And
    %% Coauthor \\
    %% Affiliation \\
    %% Address \\
    %% \texttt{email} \\
    %% \And
    %% Coauthor \\
    %% Affiliation \\
    %% Address \\
    %% \texttt{email} \\
}

% Uncomment to remove the date
%\date{}

 %Uncomment to override  the `A preprint' in the header
\renewcommand{\headeright}{Technical Report}
\renewcommand{\undertitle}{Technical Report}
\renewcommand{\shorttitle}{\textit{Annual Report}}

%%% Add PDF metadata to help others organize their library
%%% Once the PDF is generated, you can check the metadata with
%%% $ pdfinfo template.pdf
\hypersetup{
pdftitle={Closed-Loop GPU Kernel Optimization Using Large Language Models},
pdfsubject={},
pdfauthor={Simar Rekhi},
pdfkeywords={GPU kernels, compiler optimization, large language models, Triton, performance tuning}
}

\begin{document}
\maketitle

\begin{abstract}
    GPU kernel optimization is a critical challenge in modern high-performance computing, requiring deep domain expertise and extensive empirical tuning to achieve optimal performance across hardware platforms. While compiler frameworks and automatic code generators have made significant progress, they are constrained by predefined search spaces, hand-crafted heuristics, and fixed optimization templates. Recent advances in large language models (LLMs) suggest a complementary approach for reasoning about code and optimization parameters, but naïve one-shot generation often lacks the domain grounding and iterative refinement required for high-performance GPU programming.
    \newline

    This work proposes a closed-loop architecture for LLM-assisted GPU kernel optimization that integrates large language models with program analysis, automated testing, and performance profiling. Rather than synthesizing kernels from scratch, the system operates on parameterized GPU kernels and iteratively refines optimization parameters using structured LLM prompts informed by kernel source code, hardware characteristics, and historical optimization outcomes. A persistent knowledge archive captures correctness and performance data across optimization runs, enabling feedback-driven refinement and reuse of prior results.
    \newline

    We implement the proposed framework using Triton kernels and demonstrate its effectiveness on matrix multiplication and softmax workloads on a CUDA-enabled GPU. Experimental results show that the closed-loop system can identify parameter configurations that significantly improve performance over baseline implementations within a small number of iterations, while maintaining correctness through automated validation. This work illustrates how structured feedback loops can ground LLM-driven optimization in empirical performance measurements, providing a practical pathway for integrating language models into GPU kernel optimization workflows.
\end{abstract}

\section{Introduction}
Graphics Processing Units (GPUs) underpin many of the performance breakthroughs in machine learning, scientific simulation and data analytics. Efficient GPU kernels are typically small, highly optimized programs that run on these accelerators and are essential to unlocking their computational potential. However, fabricating these high-performance kernels remains notoriously difficult because of the low-level determinism of GPU execution and the heavy dependence on implementation-specific techniques. Developers must understand intricate hardware details, tune dozens of parameters and account for subtle memory and execution patterns. Compiler frameworks and automatic code generators have steadily improved productivity by transforming high-level knowledge bases into tuned kernels. Yet these systems are bounded by the templates and search spaces they encode. 

In parallel, large language models (LLMs) have emerged as powerful tools for code synthesis and transformation. Their ability to ingest vast quantities of source code suggests a path toward automated kernel generation without hand-crafted heuristics. Initial experiments have shown that LLMs can produce plausible GPU kernels, but they often lack the specialized domain knowledge required for true high performance and are prone to producing incorrect or sub-optimal code. Moreover, current approaches typically treat code generation as a one-shot problem, omitting the iterative refinement and feedback that expert programmers rely on when tuning kernels. 


\section{Related Work}
The following peer-reviewed papers provide representative approaches to automatic GPU kernel generation. TVM (OSDI 2018) introduces a tensor compiler that automatically generates optimized code for diverse hardware by combining graph‑level and operator‑level transformations with a learned cost model. FlexTensor (ASPLOS 2020) explores a larger schedule space using heuristic and machine‑learning techniques, achieving notable speedups on CPUs, GPUs and FPGAs. The Halide GPU autoscheduler (OOPSLA 2021) focuses on imaging pipelines, using a two‑phase search, hierarchical sampling and memoization to produce high‑performance schedules without templates. 

\subsection{Comparative Features}
Table~1 summarizes key differences between representative compiler-based kernel optimization frameworks and highlights the role of learned cost models and search strategies.\newline

\begin{table}[H]
\centering
\begin{tabular}{|
  >{\raggedright\arraybackslash}m{3.2cm}|
  >{\raggedright\arraybackslash}m{3.8cm}|
  >{\raggedright\arraybackslash}m{3.8cm}|
  >{\raggedright\arraybackslash}m{3.8cm}|}
\hline
\textbf{Feature} &
\href{https://www.usenix.org/system/files/osdi18-chen.pdf}{\textbf{TVM}} &
\href{https://ceca.pku.edu.cn/docs/20200915213803856105.pdf}{\textbf{FlexTensor}} &
\href{https://cseweb.ucsd.edu/~tzli/gpu_autoscheduler.pdf}{\textbf{Halide GPU Autoscheduler}}\\
\hline
Knowledge construction &
Builds a search space of loop tilings, fusion rules and tensorization. Trains a learned cost model on measured performance to predict promising schedules. &
Constructs a comprehensive schedule design space and uses heuristics plus reinforcement learning to explore it. Performance data are stored and used to guide search. &
Explores a large set of schedules via hierarchical sampling. Uses a cost model blending program analysis, ML predictions and hardware characteristics. Memoizes partial schedules.\\
\hline
Hardware generalization &
Separates high‑level program representation from hardware‑specific schedules; uses tensorization rules and per‑backend cost models to tune for CPUs, GPUs and accelerators. &
Parameterizes the search by hardware (cores, memory hierarchy) so the RL agent adapts schedules to CPUs, GPUs and FPGAs, delivering cross‑platform speedups. &
Uses a platform‑independent representation; the cost model encodes GPU parameters. Although focused on CUDA GPUs, the approach could extend to other accelerators by adjusting the cost model.\\
\hline
Handling fused kernels &
Supports operator fusion at the IR level; the cost model evaluates fused operators and the search explores fusion possibilities in a hardware‑agnostic way. &
Primarily optimizes individual operators and small compositions; does not explicitly target long‑range fusion across many stages. &
Considers fusing stages as part of the scheduling space; hierarchical grouping and memoization manage fusion diversity and enable automatic generation of fused kernels.\\
\hline
\end{tabular}
\end{table}

\section{Problem Statement and Scope}

Optimizing GPU kernels remains a fundamental challenge in high-performance computing. Achieving near-optimal performance requires careful selection of low-level parameters such as block sizes, warp counts, memory access patterns, and pipeline depth, all of which interact in complex, hardware-dependent ways. While modern compiler frameworks and domain-specific languages expose tunable parameters and automated search mechanisms, their effectiveness is constrained by predefined search spaces, hand-crafted heuristics, and cost models that must be explicitly engineered and maintained.

Recent advances in large language models (LLMs) suggest a complementary paradigm in which optimization decisions are informed by learned representations of code and performance-relevant patterns. However, existing approaches that apply LLMs to GPU programming often treat kernel generation or optimization as a one-shot task, without systematic feedback from compilation, correctness validation, or empirical performance measurement. As a result, generated kernels may be syntactically plausible but inefficient, incorrect, or unstable across inputs.

This work addresses the following problem: \emph{how can large language models be integrated into GPU kernel optimization workflows in a way that enables iterative improvement, preserves correctness, and grounds optimization decisions in measured performance rather than static heuristics?}

To this end, we propose a closed-loop optimization framework in which an LLM acts as a decision-making agent over a structured parameter space, rather than as a standalone code generator. The system operates on existing parameterized GPU kernels and iteratively refines optimization parameters using feedback from automated compilation, correctness testing, and performance profiling. A persistent knowledge archive records optimization outcomes across iterations, enabling feedback-driven refinement and reuse of prior results.

The scope of this work is intentionally constrained. The proposed framework targets parameter optimization and schedule refinement of pre-existing kernels rather than full kernel synthesis from scratch. The evaluation focuses on a limited set of kernels implemented in Triton and executed on a single GPU architecture. Within this scope, the objective is not to outperform state-of-the-art compiler frameworks universally, but to demonstrate that structured feedback loops can effectively ground LLM-driven optimization in empirical evidence and produce meaningful performance improvements without manual tuning.



\section{Proposed Approach}
\subsection{Motivation}

Large language models can synthesize GPU kernel code but often lack the domain knowledge to ensure correctness and performance. A closed-loop system can compensate by incorporating program analysis, compilation and feedback. The literature highlights several ideas to borrow: 
\begin{itemize}
    \item {Constructing a rich search space of transformations}
    \item{Using cost models trained on past executions}
    \item{Separating hardware-independent specifications from hardware-specific schedules. Our proposed design integrates these concepts with LLMs. }
\end{itemize}

\subsection{Architecture Overview}

The system maintains a persistent knowledge archive of kernel implementations, optimization parameters, performance metrics and hardware profiles. The architecture is organized into several modular components that work together in a closed-loop optimization cycle. The system comprises the following components:

\textbf{Core Components:}
\begin{itemize}
    \item {\textbf{Baseline Framework:} Provides reference PyTorch implementations for benchmarking and correctness validation. Supports automatic input generation and statistical performance measurement across multiple runs.}
    \item {\textbf{Triton Kernel Library:} Contains parameterized kernel implementations with tunable optimization parameters. Kernel source code is stored in isolated files to enable safe code reading without triggering inspection of compiled JITFunction objects.}
    \item {\textbf{Testing Framework:} Automates correctness validation against PyTorch baselines using configurable numerical tolerance, and measures performance with warmup, multiple runs and statistical analysis (median, mean, min, max runtime).}
    \item {\textbf{Knowledge Archive:} Persistently stores all optimization attempts, including kernel code, parameters, speedup metrics, correctness flags, runtime measurements and metadata. Provides query interface for retrieving best configurations and optimization history.}
    \item {\textbf{LLM Optimizer:} Constructs structured prompts containing kernel source code, hardware characteristics, current parameters, tunable parameter ranges and optimization history. Interfaces with OpenAI API for parameter suggestions, with heuristic fallback when API is unavailable.}
    \item {\textbf{Reporter:} Generates comprehensive optimization reports including parameter impact analysis, stability testing across input sizes, and documentation of findings.}
\end{itemize}

The closed loop comprises the following stages:
\begin{enumerate}
    \item {\textbf{Baseline Establishment:} Initialize with default kernel parameters. Measure baseline performance using PyTorch reference implementations. Store baseline configuration and metrics in the knowledge archive. Establish correctness threshold based on numerical precision of the target data type (e.g., 0.1 for float16 operations).}

    \item {\textbf{Kernel Code Retrieval:} Load kernel source code from isolated file storage. The system uses a dedicated kernel code reader module that reads source files directly, avoiding any inspection of compiled kernel objects. This enables safe code extraction for LLM prompts without triggering inspection errors.}

    \item {\textbf{Prompt Construction:} Build a structured prompt for the LLM containing:
    \begin{itemize}
        \item {Complete kernel source code loaded from file}
        \item {Target hardware characteristics (GPU architecture, compute capability, memory, CUDA version)}
        \item {Current parameter values and valid ranges for tunable parameters}
        \item {Summary of previous optimization attempts including parameters, speedup and correctness outcomes}
        \item {Performance goals and constraints}
    \end{itemize}
    This comprehensive context grounds the LLM in the optimization problem and enables informed parameter suggestions.}

    \item {\textbf{Parameter Suggestion:} Query the LLM with the structured prompt to generate new parameter values. The LLM analyzes the kernel implementation, considers hardware characteristics and learns from previous attempts. Parameters are validated against valid ranges and adjusted to nearest valid values if needed. When LLM API is unavailable, the system falls back to heuristic-based exploration strategies.}

    \item {\textbf{Kernel Compilation and Execution:} Compile the Triton kernel with suggested parameters. The kernel is automatically compiled by the Triton JIT compiler with the new parameter values. Execute on representative inputs generated by the testing framework.}

    \item {\textbf{Correctness Verification:} Compare kernel outputs against PyTorch reference implementations across multiple random inputs. Calculate maximum numerical error. Accept kernel if error is below tolerance threshold (accounting for float16 precision limitations). Record error metrics and correctness status.}

    \item {\textbf{Performance Measurement:} Measure runtime with warmup period followed by multiple runs (typically 100 runs). Compute statistical metrics including median, mean, minimum and maximum runtime. Calculate speedup relative to baseline implementation.}

    \item {\textbf{Archive Storage:} Store complete optimization attempt in knowledge archive including kernel code, parameters, speedup, correctness status, runtime metrics, baseline comparison and timestamp. Update metadata statistics tracking total attempts and success rates.}

    \item {\textbf{Feedback and Iteration:} Append optimization attempt to history. If result improves upon best known configuration, update best parameters and continue from new baseline. If result fails correctness or degrades performance, revert to previous best configuration. Formulate feedback for next LLM query based on correctness, speedup trends and error patterns.}

    \item {\textbf{Termination and Reporting:} Continue loop for specified maximum iterations or until speedup threshold is achieved (e.g., 2x speedup). Early stopping prevents wasted computation once diminishing returns are reached. Generate comprehensive optimization report including best configuration, parameter impact analysis, stability assessment across input sizes and optimization statistics.}
\end{enumerate}


\graphicspath{{}}
 \begin{figure}[htbp]
        \centering
        \includegraphics[width=0.8\textwidth]{closed_loop.png}
        \caption{Illustrates the end-to-end closed-loop workflow and the interaction between system components.}
        \label{fig:visual}
    \end{figure}



\subsection{Closed-Loop Optimization Procedure}
The closed-loop optimization process can be summarized by the following high-level pseudocode, illustrating the interaction between the LLM, testing framework, and knowledge archive:\newline

\begin{Verbatim}[formatcom=\color{mycodecolor}]
Initialize optimizer with kernel name, target device, and iteration budget
Load parameterized kernel source code from isolated storage
Retrieve tunable parameter ranges and default configuration

Evaluate baseline configuration:
    Execute kernel with default parameters
    Verify correctness against reference implementation
    Measure baseline runtime and record results
    Store baseline entry in knowledge archive

Set current configuration ← baseline configuration
Set best configuration ← baseline configuration

For each optimization iteration:
    Construct structured prompt including:
        - Kernel source code
        - Hardware characteristics
        - Current parameter values
        - Valid parameter ranges
        - Historical optimization outcomes

    Query LLM to propose new parameter configuration
    Validate proposed parameters against allowable ranges

    Compile and execute kernel with proposed parameters
    Verify correctness using automated testing

    If correctness check passes:
        Measure runtime and compute speedup relative to baseline
    Else:
        Assign zero speedup and record failure

    Store optimization attempt and metrics in knowledge archive

    If proposed configuration improves best-known speedup:
        Update best configuration
        Set current configuration ← proposed configuration
    Else:
        Revert current configuration ← best configuration

    If target speedup or iteration limit is reached:
        Terminate optimization loop

Generate optimization report summarizing:
    - Best-performing configuration
    - Parameter impact analysis
    - Stability and correctness statistics
\end{Verbatim}

This abstraction emphasizes the closed-loop nature of the system while remaining independent of any specific programming language or implementation details.

\section {Implementation}

The proposed closed-loop framework is implemented as a modular Python system organized into several components. The \texttt{KernelOptimizer} class orchestrates the optimization process, coordinating interactions between the testing framework, knowledge archive, LLM optimizer and reporting system. Kernel source code is stored in isolated files within a dedicated directory structure, enabling safe code extraction without triggering inspection of compiled kernel objects. An isolated kernel code reader module provides the sole interface for loading kernel source code, maintaining separation from the kernel execution environment. The testing framework implements automated correctness validation with configurable tolerance thresholds appropriate for different numerical precisions, along with statistical performance measurement including warmup periods and multiple execution runs. The knowledge archive maintains persistent JSON storage of all optimization attempts, enabling query-based retrieval of best configurations and historical analysis. The LLM optimizer constructs structured prompts incorporating kernel source code, hardware profiles and optimization history, interfacing with OpenAI's API while providing heuristic fallback for parameter suggestion when API access is unavailable. The reporter generates comprehensive optimization reports including parameter impact analysis and stability assessment across different input sizes.

\section{Experimental Evaluation}

This section evaluates the proposed closed-loop LLM-assisted kernel optimization framework in a realistic GPU programming setting. The evaluation focuses on three core questions: (1) whether the system can automatically discover performance-improving parameter configurations, (2) whether correctness can be preserved through automated validation, and (3) how stable and generalizable the discovered optimizations are across input sizes. All experiments are conducted using the implementation provided in the accompanying public repository.

\subsection{Setup and Methodology}

Experiments are conducted on an NVIDIA Tesla T4 GPU with 16~GB of memory and CUDA~12.6, using Google Colab as the execution environment. The system targets Triton-based GPU kernels with explicitly exposed tunable parameters. For correctness validation, PyTorch reference implementations are used as ground truth. All kernels operate on FP16 inputs, and numerical correctness is verified using a maximum absolute error tolerance of 0.1, accounting for reduced precision arithmetic.

For each kernel, the optimization process begins by establishing a baseline configuration using default Triton parameters. Baseline performance is measured relative to the PyTorch reference implementation and recorded in the persistent knowledge archive. The closed-loop optimizer then executes up to 20 optimization iterations. In each iteration, a structured prompt containing kernel source code, hardware characteristics, current parameters, valid parameter ranges, and historical optimization outcomes is provided to the LLM. When an API key is unavailable, the system falls back to heuristic parameter exploration.

Each proposed parameter configuration is validated against allowable ranges, compiled using the Triton JIT compiler, and executed on representative randomly generated inputs. Correctness is verified across five test cases per configuration. Performance measurement includes 10 warmup runs followed by 100 timed executions, with statistical metrics (median, mean, minimum, and maximum runtime) recorded. Speedup is computed relative to the baseline configuration and all results are persisted in the knowledge archive.

\subsection{Benchmarks}

The evaluation considers two representative GPU kernels: matrix multiplication (matmul) and softmax. These kernels are widely used in machine learning workloads and expose multiple tunable parameters that influence memory access patterns, parallelism, and resource utilization.

The matrix multiplication kernel is parameterized by block sizes in the M, N, and K dimensions (BLOCK\_SIZE\_M, BLOCK\_SIZE\_N, BLOCK\_SIZE\_K), group size for program ID mapping (GROUP\_SIZE\_M), pipeline stages (num\_stages), and the number of warps (num\_warps). The softmax kernel exposes a smaller set of tunable parameters, primarily block size and warp configuration. For both kernels, baseline performance is established using PyTorch’s optimized FP16 implementations.

\subsection{Optimization Results}

Unless otherwise stated, all reported speedups are computed relative to the default Triton baseline configuration, not the PyTorch reference implementation.\newline

For the matrix multiplication kernel, the baseline Triton configuration with default parameters achieved performance comparable to PyTorch but left substantial room for improvement. Across optimization iterations, the LLM proposed diverse parameter configurations exploring variations in block sizes, warp counts, grouping strategies, and pipeline depth. Within three to five iterations, the system identified configurations yielding speedups between 2$\times$ and 3$\times$ over the default Triton baseline.

Successful configurations typically increased BLOCK\_SIZE\_M and BLOCK\_SIZE\_N while selecting warp counts that balanced occupancy and register pressure. Pipeline stages were adjusted to maximize arithmetic intensity without introducing numerical instability. The persistent knowledge archive enabled the optimizer to avoid previously unsuccessful configurations and focus exploration on promising regions of the parameter space.

For the softmax kernel, performance improvements were more modest due to a smaller optimization space. Nevertheless, block size tuning yielded speedups of approximately 1.5$\times$ within a small number of iterations. All optimized configurations for both kernels passed correctness verification, with numerical errors remaining below the specified tolerance threshold.

\subsection{Stability and Generalization}

To assess stability, the best-performing configurations were evaluated across varying input sizes. For matrix multiplication, the optimized configuration demonstrated correct execution and improved performance for a $512 \times 512 \times 512$ problem size, achieving a speedup of approximately 0.199× relative to the default Triton baseline. However, the same configuration failed correctness checks for larger input sizes, including $1024 \times 1024 \times 1024$, $2048 \times 2048 \times 2048$, and $4096 \times 4096 \times 4096$.

These results indicate that while the closed-loop system can effectively optimize kernels for specific input regimes, parameter configurations do not necessarily generalize across problem sizes without additional tuning. This limitation highlights the inherent difficulty of achieving performance portability and motivates future work on input-size-aware optimization and multi-objective tuning strategies.

\subsection{Discussion}

The experimental results demonstrate that integrating large language models into a closed-loop optimization framework enables effective exploration of GPU kernel parameter spaces without manual tuning. By grounding LLM suggestions in complete kernel source code, hardware characteristics, and empirical performance feedback, the system discovers non-trivial optimization configurations within a small number of iterations. Automated correctness validation ensures that invalid configurations are discarded early, preventing wasted computation.

The persistent knowledge archive plays a central role in guiding exploration, enabling the system to learn from prior successes and failures across optimization runs. While the current implementation focuses on parameter optimization for existing Triton kernels, the observed improvements suggest that LLMs can serve as effective decision-making agents when embedded within structured, feedback-driven systems. The limitations observed in generalization underscore the need for further research into adaptive parameterization and cross-input optimization, but do not diminish the effectiveness of the proposed approach within constrained optimization settings.\newline 

Compared with purely-search based compilers like TVM or FlexTensor, the LLM-assisted system offers several advantages. The LLM can synthesize parameter combinations that may lie outside predetermined search spaces, drawing on its training data that includes diverse optimization patterns. By providing complete kernel source code, hardware characteristics and optimization history in structured prompts, the LLM can apply domain knowledge about memory coalescing, occupancy optimization and pipeline tuning. The closed loop ensures that incorrect suggestions are detected early through compilation and correctness testing before performance measurement, preventing wasted computation. Measured speedup acts as an explicit feedback signal that guides subsequent LLM parameter suggestions. The persistent knowledge archive continuously expands with each optimization run, enabling future sessions to learn from past successes and failures, creating an feedback-driven system that improves over time. The modular architecture with isolated kernel code reading, dedicated testing framework and comprehensive reporting enable a more adaptive and empirically grounded kernel optimization process. 

\section{Limitations and Future Scope}

While the proposed closed-loop framework demonstrates the feasibility of LLM-assisted GPU kernel optimization, several limitations should be acknowledged.

First, the current implementation operates on a restricted set of parameterized kernels and does not perform full kernel synthesis or algorithmic transformation. The LLM is limited to selecting from predefined parameter ranges exposed by the kernel implementation, which bounds the expressiveness of the optimization space. Extending the framework to support structural kernel transformations or multi-kernel fusion would require additional safeguards for correctness and substantially more complex validation mechanisms.

Second, experimental evaluation is conducted on a single GPU architecture and a limited set of workloads. As observed in the stability analysis, parameter configurations optimized for a specific input size do not necessarily generalize to larger problem dimensions. This highlights an inherent challenge in GPU optimization and indicates that additional mechanisms, such as input-size-aware tuning or multi-objective optimization across workload distributions, are necessary to achieve broader performance portability.

Third, the framework relies on empirical performance measurements, which incur non-trivial computational cost. Although the closed-loop design mitigates wasted computation by rejecting incorrect configurations early, scaling the approach to larger kernels or more expansive parameter spaces may require integration with learned cost models or surrogate performance predictors to reduce evaluation overhead.

Finally, the behavior of the system is influenced by the capabilities and limitations of the underlying language model. While structured prompting and feedback significantly improve reliability over one-shot generation, LLM suggestions may still exhibit bias toward common patterns observed in training data. Future work may explore hybrid approaches that combine LLM-driven reasoning with traditional compiler analyses, reinforcement learning, or symbolic performance models.

Despite these limitations, the framework establishes a foundation for integrating language models into performance-critical compilation workflows. Future directions include expanding the set of supported kernels, incorporating adaptive parameter ranges, improving generalization across input sizes and hardware platforms, and exploring deeper integration between LLMs and compiler internals. These extensions would further strengthen the role of feedback-driven learning in automated GPU kernel optimization.



\section {Conclusion}
Automatic GPU kernel generation is a challenging problem that requires both algorithmic insight and hardware-specific knowledge. Peer-reviewed systems such as TVM, FlexTensor and the Halide GPU autoscheduler demonstrate that domain-specific compilers with learned cost models can achieve performance competitive with hand-tuned kernels across multiple hardware backends. However, these systems rely on fixed search spaces and cannot easily explore additional configurations. Large language models have the capacity to synthesize proposed code but must be grounded through domain knowledge and iterative feedback. A closed-loop architecture that integrates LLMs with compilation, testing and performance measurement can leverage the strengths of both approaches. The modular implementation provided here, featuring isolated kernel code management, automated testing frameworks, persistent knowledge archives and comprehensive reporting systems, demonstrates an empirically validated approach to kernel optimization. The file-based kernel code reading system ensures safe operation while the knowledge archive creates a feedback-driven system that improves through accumulated experience. This work underscores the importance of combining machine learning, program analysis and human knowledge in the pursuit of efficient kernel generation, while demonstrating that structured feedback loops can effectively guide LLMs toward high-performance solutions.

\bibliographystyle{unsrtnat}
\begin{thebibliography}{6}

\bibitem{chen2018tvm}
Tianqi~Chen, Thierry~Moreau, Ziheng~Jiang, Lianmin~Zheng, Eddie~Yan,
Haichen~Shen, Megan~Cowan, Leyuan~Wang, Yuwei~Hu, Junru~Liu, Luis~Ceze,
Carlos~Guestrin and Arvind~Krishnamurthy.
\newblock TVM: An Automated End-to-End Optimizing Compiler for Deep Learning.
\newblock In \emph{Proceedings of the 13th USENIX Symposium on Operating Systems Design and Implementation (OSDI)}, pages 578–594. USENIX Association, 2018.

\bibitem{tian2020flextensor}
Jiannan~Tian, Liwen~Zhang, Tian~Xie, Tianqi~Chen, Yuwei~Hu, Mingkui~Tan, Yufei~Ding and Xipeng~Shen.
\newblock FlexTensor: An Automatic Schedule Exploration and Optimization Framework for Tensor Computation on Heterogeneous Systems.
\newblock In \emph{Proceedings of the 25th International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS)}, pages 859–873. ACM, 2020.

\bibitem{anderson2021autoscheduler}
Ian~C.~Anderson, Tianqi~Chen, William~Moses, Amara~Henao, Joshua~Triplett, Chiyuan~Zhang, Matei~Zaharia and Jonathan~Ragan-Kelley.
\newblock Efficient Automatic Scheduling of Imaging and Vision Pipelines for the GPU.
\newblock In \emph{Proceedings of the ACM on Programming Languages (OOPSLA)}, volume~5, number~OOPSLA, pages 1–26. ACM, 2021.

\bibitem{rekhi2025compiler}
Simar~Rekhi.
\newblock Compiler Pass Generation – Triton Kernel Optimizer.
\newblock GitHub repository, accessed 2025. Available at \url{https://github.com/simar-rekhi/compiler-pass-generation}.

\end{thebibliography}
\end{document}
